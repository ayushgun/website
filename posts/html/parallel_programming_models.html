<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="../../stylesheets/styles.css" />
    <style>
      pre {
        line-height: 125%;
      }
      td.linenos .normal {
        color: inherit;
        background-color: transparent;
        padding-left: 5px;
        padding-right: 5px;
      }
      span.linenos {
        color: inherit;
        background-color: transparent;
        padding-left: 5px;
        padding-right: 5px;
      }
      td.linenos .special {
        color: #000000;
        background-color: #ffffc0;
        padding-left: 5px;
        padding-right: 5px;
      }
      span.linenos.special {
        color: #000000;
        background-color: #ffffc0;
        padding-left: 5px;
        padding-right: 5px;
      }
      .codehilite .hll {
        background-color: #ffffcc;
      }
      .codehilite {
        background: #ffffff;
      }
      .codehilite .c {
        color: #888;
        font-style: normal;
      } /* Comment */
      .codehilite .err {
        background-color: #a848a8;
      } /* Error */
      .codehilite .k {
        color: #2838b0;
      } /* Keyword */
      .codehilite .o {
        color: #666;
      } /* Operator */
      .codehilite .p {
        color: #888;
      } /* Punctuation */
      .codehilite .ch {
        color: #287088;
        font-style: normal;
      } /* Comment.Hashbang */
      .codehilite .cm {
        color: #888;
        font-style: normal;
      } /* Comment.Multiline */
      .codehilite .cp {
        color: #289870;
      } /* Comment.Preproc */
      .codehilite .cpf {
        color: #888;
        font-style: normal;
      } /* Comment.PreprocFile */
      .codehilite .c1 {
        color: #888;
        font-style: normal;
      } /* Comment.Single */
      .codehilite .cs {
        color: #888;
        font-style: normal;
      } /* Comment.Special */
      .codehilite .gd {
        color: #c02828;
      } /* Generic.Deleted */
      .codehilite .ge {
        font-style: normal;
      } /* Generic.Emph */
      .codehilite .ges {
        font-weight: normal;
        font-style: normal;
      } /* Generic.EmphStrong */
      .codehilite .gr {
        color: #c02828;
      } /* Generic.Error */
      .codehilite .gh {
        color: #666;
      } /* Generic.Heading */
      .codehilite .gi {
        color: #388038;
      } /* Generic.Inserted */
      .codehilite .go {
        color: #666;
      } /* Generic.Output */
      .codehilite .gp {
        color: #444;
      } /* Generic.Prompt */
      .codehilite .gs {
        font-weight: normal;
      } /* Generic.Strong */
      .codehilite .gu {
        color: #444;
      } /* Generic.Subheading */
      .codehilite .gt {
        color: #2838b0;
      } /* Generic.Traceback */
      .codehilite .kc {
        color: #444;
        font-style: normal;
      } /* Keyword.Constant */
      .codehilite .kd {
        color: #2838b0;
        font-style: normal;
      } /* Keyword.Declaration */
      .codehilite .kn {
        color: #2838b0;
      } /* Keyword.Namespace */
      .codehilite .kp {
        color: #2838b0;
      } /* Keyword.Pseudo */
      .codehilite .kr {
        color: #2838b0;
      } /* Keyword.Reserved */
      .codehilite .kt {
        color: #2838b0;
        font-style: normal;
      } /* Keyword.Type */
      .codehilite .m {
        color: #444;
      } /* Literal.Number */
      .codehilite .s {
        color: #b83838;
      } /* Literal.String */
      .codehilite .na {
        color: #388038;
      } /* Name.Attribute */
      .codehilite .nb {
        color: #388038;
      } /* Name.Builtin */
      .codehilite .nc {
        color: #287088;
      } /* Name.Class */
      .codehilite .no {
        color: #b85820;
      } /* Name.Constant */
      .codehilite .nd {
        color: #287088;
      } /* Name.Decorator */
      .codehilite .ni {
        color: #709030;
      } /* Name.Entity */
      .codehilite .ne {
        color: #908828;
      } /* Name.Exception */
      .codehilite .nf {
        color: #785840;
      } /* Name.Function */
      .codehilite .nl {
        color: #289870;
      } /* Name.Label */
      .codehilite .nn {
        color: #289870;
      } /* Name.Namespace */
      .codehilite .nt {
        color: #2838b0;
      } /* Name.Tag */
      .codehilite .nv {
        color: #b04040;
      } /* Name.Variable */
      .codehilite .ow {
        color: #a848a8;
      } /* Operator.Word */
      .codehilite .pm {
        color: #888;
      } /* Punctuation.Marker */
      .codehilite .w {
        color: #a89028;
      } /* Text.Whitespace */
      .codehilite .mb {
        color: #444;
      } /* Literal.Number.Bin */
      .codehilite .mf {
        color: #444;
      } /* Literal.Number.Float */
      .codehilite .mh {
        color: #444;
      } /* Literal.Number.Hex */
      .codehilite .mi {
        color: #444;
      } /* Literal.Number.Integer */
      .codehilite .mo {
        color: #444;
      } /* Literal.Number.Oct */
      .codehilite .sa {
        color: #444;
      } /* Literal.String.Affix */
      .codehilite .sb {
        color: #b83838;
      } /* Literal.String.Backtick */
      .codehilite .sc {
        color: #a848a8;
      } /* Literal.String.Char */
      .codehilite .dl {
        color: #b85820;
      } /* Literal.String.Delimiter */
      .codehilite .sd {
        color: #b85820;
        font-style: normal;
      } /* Literal.String.Doc */
      .codehilite .s2 {
        color: #b83838;
      } /* Literal.String.Double */
      .codehilite .se {
        color: #709030;
      } /* Literal.String.Escape */
      .codehilite .sh {
        color: #b83838;
      } /* Literal.String.Heredoc */
      .codehilite .si {
        color: #b83838;
        text-decoration: underline;
      } /* Literal.String.Interpol */
      .codehilite .sx {
        color: #a848a8;
      } /* Literal.String.Other */
      .codehilite .sr {
        color: #a848a8;
      } /* Literal.String.Regex */
      .codehilite .s1 {
        color: #b83838;
      } /* Literal.String.Single */
      .codehilite .ss {
        color: #b83838;
      } /* Literal.String.Symbol */
      .codehilite .bp {
        color: #388038;
        font-style: normal;
      } /* Name.Builtin.Pseudo */
      .codehilite .fm {
        color: #b85820;
      } /* Name.Function.Magic */
      .codehilite .vc {
        color: #b04040;
      } /* Name.Variable.Class */
      .codehilite .vg {
        color: #908828;
      } /* Name.Variable.Global */
      .codehilite .vi {
        color: #b04040;
      } /* Name.Variable.Instance */
      .codehilite .vm {
        color: #b85820;
      } /* Name.Variable.Magic */
      .codehilite .il {
        color: #444;
      } /* Literal.Number.Integer.Long */
      .codehilite {
        background: transparent;
      }
    </style>
    <title>Ayush Gundawar &mdash; Post</title>
  </head>
  <body>
    <div class="container">
      <div class="content">
        <h1>Parallel Programming Models</h1>
        <blockquote>
          <p>April 5, 2025</p>
        </blockquote>
        <p>
          Yesterday, I spoke at the
          <a href="https://dtyped.netlify.app"
            >Programming Languages Club at Georgia Tech</a
          >
          about exposing models for parallelism in programming languages. You
          can find the slides
          <a
            href="https://cdn.discordapp.com/attachments/1345041727899176980/1357803322240798830/Parallel_Programming_Models.pdf?ex=67f2d9e2&amp;is=67f18862&amp;hm=c9cdad2eafb1d22665ed424a99d117c9f59208354c68d713c0499b24cc81d28c&amp;"
            >here</a
          >. This post is a synopsis of the content I went over in my talk.
        </p>
        <h2>Motivation for Parallelism</h2>
        <p>
          In the early days of computing, improving performance was
          straightforward: make the processor faster. During the 1990s, chip
          manufacturers simply shrank transistors with each generation, as
          described by Moore's Law (transistor density doubling roughly every
          two years). Smaller transistors meant more of them could fit in the
          same area and signals had shorter distances to travel, yielding higher
          clock speeds and lower power per operation. For a while, clock speeds
          and transistor counts increased hand-in-hand, making programs run
          faster without any changes to software.
        </p>
        <p>
          However, this approach hit physical limits. As transistors got denser,
          chips ran into problems with heat dissipation and manufacturing
          complexity (e.g. the challenges of 3nm and smaller process nodes).
          Chip designers reached a point of diminishing returns: simply adding
          more transistors or upping the clock rate began to yield less
          improvement, while generating more heat and cost. You can't
          indefinitely get more juice by squeezing the same lemon.
        </p>
        <p>
          The industry's response in the mid-2000s was to scale out horizontally
          by adding multiple processing cores on a single chip. Instead of one
          super-fast core, you get several moderately fast cores working in
          parallel. For example, an Apple M1 Pro has 8 cores, and high-end
          server processors (like Intel Xeon) have up to 144 cores. Each core
          can run tasks in parallel independently, so an application can
          <em>potentially</em> run N times faster with N cores (if the work can
          be evenly divided).
        </p>
        <h3>Different Forms of Parallelism</h3>
        <ol>
          <li>
            Data Parallelism: performing the same operation on different pieces
            of data simultaneously (e.g. adding two arrays element-wise in
            parallel).
          </li>
          <li>
            Task Parallelism: running independent tasks or functions in
            parallel, which may be doing different things (e.g. a web server
            handling multiple requests on different threads).
          </li>
          <li>
            Device Parallelism: spreading work across multiple physical devices
            (like using multiple servers, or offloading some tasks to a GPU or
            TPU). This can be seen as a coarse-grained form of task parallelism
            across machines or specialized processors.
          </li>
        </ol>
        <p>
          Modern high-performance computing often combines these forms. In this
          post, we'll mostly concentrate on data parallelism – making an
          algorithm execute faster by applying it in parallel on many data
          elements.
        </p>
        <h2>SIMD</h2>
        <p>
          Adding more cores isn't the only way to speed up computation; we can
          also make each core itself do more work in parallel. Modern CPUs
          already exploit a lot of internal parallelism with techniques like
          instruction pipelining, out-of-order execution, and speculative
          execution. Pipelining, for instance, splits the execution of an
          instruction into stages (fetch, decode, execute, etc.) so that
          multiple instructions are in progress concurrently (like an assembly
          line).
        </p>
        <p>
          SIMD is a form of internal data parallelism at the instruction level.
          The CPU has special vector registers that are, say, 128, 256, or 512
          bits wide, which can be viewed as containing multiple lanes (chunks)
          of smaller data elements. A single SIMD instruction operates on all
          lanes in one go. For example, with 128-bit NEON registers on an ARM
          processor, each register can hold four 32-bit integers (4 * 32 = 128
          bits). One SIMD add instruction can then add four pairs of integers at
          once (as opposed to four separate scalar add instructions).
        </p>
        <p>
          Under the hood, using SIMD means utilizing these wide registers and
          the corresponding instruction set extensions (SSE, AVX on x86, NEON on
          ARM, etc.). High-level languages usually expose SIMD through
          intrinsics – special macros that map directly to SIMD instructions.
          For instance, here's a simple example using ARM NEON intrinsics to add
          two vectors of integers:
        </p>
        <div class="codehilite">
          <pre><span></span><code><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;arm_neon.h&gt;</span>

<span class="c1">// 128-bit NEON registers: can hold 4 int32 values each</span>
<span class="n">int32x4_t</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">};</span>
<span class="n">int32x4_t</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">20</span><span class="p">,</span><span class="w"> </span><span class="mi">30</span><span class="p">,</span><span class="w"> </span><span class="mi">40</span><span class="p">};</span>

<span class="c1">// Use a NEON intrinsic to add all four lanes in one instruction</span>
<span class="n">int32x4_t</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vaddq_s32</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="n">b</span><span class="p">);</span>
</code></pre>
        </div>

        <p>
          Here, <code>vaddq_s32</code> is a SIMD addition that processes four
          32-bit integers per register in one instruction. The
          <code>_q</code> in the name indicates a 128-bit quadword operation
          (four lanes in this case).
        </p>
        <p>
          Real-world data rarely fits perfectly into one SIMD register, so
          programmers use vector chunking (or strip-mining) to handle
          arbitrarily large arrays with SIMD. The idea is to break the data into
          chunks the size of the vector registers, process those chunks in a
          vectorized loop, and then handle any leftover elements with a scalar
          loop. For example:
        </p>
        <div class="codehilite">
          <pre><span></span><code><span class="kt">int</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="mi">1030</span><span class="p">],</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="mi">1030</span><span class="p">],</span><span class="w"> </span><span class="n">result</span><span class="p">[</span><span class="mi">1030</span><span class="p">];</span>

<span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="k">for</span><span class="w"> </span><span class="p">(;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="mi">1030</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">int32x4_t</span><span class="w"> </span><span class="n">va</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vld1q_s32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span><span class="w">      </span><span class="c1">// Load 4 elements from `a` into a SIMD vector</span>
<span class="w">  </span><span class="n">int32x4_t</span><span class="w"> </span><span class="n">vb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vld1q_s32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span><span class="w">      </span><span class="c1">// Load 4 elements from `b` into a SIMD vector</span>
<span class="w">  </span><span class="n">int32x4_t</span><span class="w"> </span><span class="n">vr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">vaddq_s32</span><span class="p">(</span><span class="n">va</span><span class="p">,</span><span class="w"> </span><span class="n">vb</span><span class="p">);</span><span class="w">     </span><span class="c1">// Element-wise addition on both vectors</span>
<span class="w">  </span><span class="n">vst1q_s32</span><span class="p">(</span><span class="o">&amp;</span><span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">vr</span><span class="p">);</span><span class="w">            </span><span class="c1">// Store contents of output vector to `result`</span>
<span class="p">}</span>

<span class="c1">// Scalar spillover loop for the remaining elements</span>
<span class="k">for</span><span class="w"> </span><span class="p">(;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1030</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</code></pre>
        </div>

        <p>
          You might wonder if compilers can do this for you automatically — the
          answer is yes, sometimes. Modern compilers can auto-vectorize simple
          loops, replacing scalar operations with SIMD instructions when it's
          safe to do so. See
          <a href="https://godbolt.org/z/efebMs1f4">this Godbolt example</a>,
          where both a scalar loop and an explicitly vectorized version compile
          down to the same <code>vpslld</code> instruction — a packed shift-left
          that multiplies four 32-bit integers in parallel.
        </p>
        <p>
          However, compilers are not omniscient. Auto-vectorization might fail
          if the code is too complex or if there are potential data dependencies
          the compiler can't prove won't overlap. In performance-critical code,
          developers still often resort to using intrinsics or inline assembly
          to explicitly vectorize algorithms when the compiler can't do it
          automatically.
        </p>
        <h2>Parallelism on CPU</h2>
        <p>
          To leverage multiple CPU cores, we use threads. A thread is the
          smallest unit of execution that can run concurrently with other
          threads while sharing the same memory space. Most languages offer an
          API for spawning threads (OS-managed threads) and some have
          higher-level abstractions (like thread pools or "green" user-space
          threads). The operating system's scheduler distributes time slices on
          CPU cores for each thread in a process. In a multi-core system,
          threads can potentially run in parallel (on different cores) – not
          just interleaved by time slicing.
        </p>
        <p>
          Creating a new OS thread is relatively expensive (it generally
          involves a system call and allocations), so you don't want to spawn
          threads frivolously. For example, suppose we want to multiply every
          element in a large array by 2 using an 8-core CPU. One naive approach
          would be to create a new thread for each element in the array:
        </p>
        <div class="codehilite">
          <pre><span></span><code><span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="mi">160000</span><span class="o">&gt;</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="cm">/*...*/</span><span class="p">};</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">();</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Spawn a thread to double `a[i]`</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="p">([</span><span class="o">&amp;</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="p">}).</span><span class="n">detach</span><span class="p">();</span>
<span class="p">}</span>
</code></pre>
        </div>

        <p>
          This is a terrible idea – it would launch 160,000 threads! The
          overhead of creating and scheduling that many threads would overwhelm
          any gain from parallelism. In fact, it would likely run slower than
          just doing the loop in one thread, due to context switch overhead and
          contention. Modern CPUs also feature Hyper-Threading (Intel) or
          similar simultaneous multithreading, where each core can run two
          hardware threads, but those still share the core's execution units.
          Hyper-threading can improve throughput for workloads that often wait
          on memory, but it doesn't double performance and can hurt
          latency-sensitive tasks. In general, you want at most one thread per
          physical core for heavy compute work.
        </p>
        <p>
          A smarter strategy for the example above is to use a fixed number of
          threads equal to the core count, and divide the array into chunks. For
          160,000 elements on 8 cores, each thread can handle 20,000 elements:
        </p>
        <div class="codehilite">
          <pre><span></span><code><span class="n">std</span><span class="o">::</span><span class="n">array</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="mi">160000</span><span class="o">&gt;</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="cm">/*...*/</span><span class="p">};</span>
<span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">core_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">8</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">chunk_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">core_count</span><span class="p">;</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">core_count</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">start</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">chunk_size</span><span class="p">;</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">end</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">start</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">chunk_size</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// Launch thread to double all elements in assigned chunk</span>
<span class="w">  </span><span class="n">threads</span><span class="p">.</span><span class="n">emplace_back</span><span class="p">([</span><span class="o">&amp;</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">start</span><span class="p">,</span><span class="w"> </span><span class="n">end</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">start</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">end</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">data</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">*=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"> </span><span class="p">}</span>
<span class="w">  </span><span class="p">});</span>
<span class="p">}</span>
</code></pre>
        </div>

        <p>
          This way, only 8 threads are created, each doing a hefty chunk of
          work. There is negligible scheduling overhead beyond those 8 threads.
          A nice bonus: a good compiler will also auto-vectorize the inner loop
          for each thread. In this case, GCC will emit a vector instruction
          (<code>vpslld</code> on x86) to multiply 4 integers at a time by 2.
        </p>
        <h3>False Sharing</h3>
        <p>
          One pitfall when parallelizing on CPUs is false sharing, which stems
          from how caches work. Each core in a modern system has its own caches,
          and memory is cached in blocks of e.g. 64 bytes called cache lines.
          When one core writes to a memory address, any other core that has that
          address cached must be alerted (cache coherence protocols will
          invalidate or update the line in other cores' caches). False sharing
          occurs when two threads on different cores are modifying independent
          variables that happen to reside on the same cache line. Each update
          ping-pongs the cache line between cores, causing stalls, even though
          the threads aren't actually sharing data in a logical sense.
        </p>
        <p>Consider this contrived example:</p>
        <div class="codehilite">
          <pre><span></span><code><span class="k">struct</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">x</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="w">  </span><span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="p">}</span><span class="w"> </span><span class="n">foo</span><span class="p">;</span>

<span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="w"> </span><span class="n">t1</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1&#39;000&#39;000</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="o">++</span><span class="n">foo</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">});</span>
<span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="w"> </span><span class="n">t2</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1&#39;000&#39;000</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="o">++</span><span class="n">foo</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">});</span>
</code></pre>
        </div>

        <p>
          Here two threads are incrementing different members of the same struct
          <code>foo</code>. Here, <code>x</code> and <code>y</code> will be on
          the same cache line. This will trigger continuous cache invalidation
          traffic between the two cores as they each modify their part of
          <code>foo</code> – thrashing the cache line back and forth.
        </p>
        <p>
          To address false sharing, we can pad or separate data so that
          concurrently modified variables reside on different cache lines. For
          example, we can insert padding bytes between <code>x</code> and
          <code>y</code>:
        </p>
        <div class="codehilite">
          <pre><span></span><code><span class="k">constexpr</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">cacheline_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">hardware_destructive_interference_size</span><span class="p">;</span>

<span class="k">struct</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">alignas</span><span class="p">(</span><span class="n">cacheline_size</span><span class="p">)</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">x</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="w">  </span><span class="k">alignas</span><span class="p">(</span><span class="n">cacheline_size</span><span class="p">)</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">atomic</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">y</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="p">}</span><span class="w"> </span><span class="n">foo</span><span class="p">;</span>

<span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="w"> </span><span class="n">t1</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1&#39;000&#39;000</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="o">++</span><span class="n">foo</span><span class="p">.</span><span class="n">x</span><span class="p">;</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">});</span>
<span class="n">std</span><span class="o">::</span><span class="kr">thread</span><span class="w"> </span><span class="n">t2</span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1&#39;000&#39;000</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="o">++</span><span class="n">foo</span><span class="p">.</span><span class="n">y</span><span class="p">;</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">});</span>
</code></pre>
        </div>

        <p>
          Most hardware uses 64-byte cache lines, and C++17 provides the
          compile-time constant
          <code>std::hardware_destructive_interference_size</code> to query the
          cache line size on an architecture. With this change,
          <code>x</code> and <code>y</code> will be aligned on different cache
          lines, so each thread mostly stays in its own cache without constantly
          invalidating the other. A simple microbenchmark on my machine shows
          the difference: without padding, the two-thread test took ~34×10^6 ns;
          with padding, ~7.1×10^6 ns. That's about a 4.8× speedup gained solely
          by eliminating false sharing.
        </p>
        <h2>Parallelism on GPU</h2>
        <p>
          While CPUs have a handful of cores optimized for sequential
          performance, GPUs have thousands of smaller cores designed for massive
          data parallelism. A GPU is essentially a compute fabric for running
          the same operation on a huge number of data elements in parallel.
          NVIDIA's CUDA framework (and similar models like OpenCL) let
          programmers launch a kernel (a function to execute on the GPU) across
          a grid of threads. The GPU hardware groups threads into blocks and
          warps for scheduling:
        </p>
        <ol>
          <li>
            A block is a group of threads that execute on the same
            multiprocessor (SM in NVIDIA terms) and can cooperate via fast
            shared memory.
          </li>
          <li>
            A grid is the collection of all blocks launched for a given kernel
            invocation. You might launch, say, 1000 blocks of 256 threads each,
            for a total of 256,000 threads executing your kernel in parallel.
          </li>
        </ol>
        <p>
          Importantly, GPU threads follow a different execution model known as
          SIMT (Single Instruction, Multiple Threads). Threads are executed in
          warps (e.g. 32 threads in NVIDIA GPUs) that proceed in lockstep on the
          same instruction. In effect, a warp is like a 32-wide SIMD unit – all
          threads run the same instruction at a time. They can have different
          data and even take different control paths, but when a warp diverges
          (e.g. an if/else where half the threads take the if-branch and half
          take else), the GPU will execute one branch first on those threads
          while the others are inactive, then the other branch. This means
          divergent branching within a warp incurs a serial execution of the
          divergent paths (affecting performance). To get best performance on
          GPUs, you want threads in a warp to execute the same path as much as
          possible. Unlike CPU threads, which are fully independent, GPU threads
          in a warp must execute in lockstep (at least at the granularity of
          warp instructions).
        </p>
        <p>
          Another key difference is memory: the CPU (host) and GPU (device) have
          separate memory spaces. Data must be transferred over relatively slow
          PCIe or NVLink buses to get to/from GPU memory. Within the GPU, memory
          access is optimized through a hierarchy: each thread has registers,
          each thread block has fast shared memory (like a user-managed L1 cache
          scratchpad), and there is large global memory (device VRAM) accessible
          to all threads. Effective GPU programming involves structuring
          computations to maximize use of fast shared memory and coalesced
          access to global memory.
        </p>
        <h3>Triton Kernels</h3>
        <p>
          Writing CUDA C++ directly can be verbose and non-portable. Triton is a
          newer approach: it's a domain-specific language embedded in Python
          that aims to simplify GPU programming while remaining close to
          hardware. Triton provides a JIT compiler that lowers your Pythonic
          kernel code through multiple stages (Triton -&gt; Triton IR -&gt; MLIR
          -&gt; PTX or other backend). Under the hood it generates code similar
          to what you'd write in CUDA, but you get to work in Python with
          auto-parallelization over a grid of threads.
        </p>
        <p>
          Below is a simple Triton kernel that multiplies every element of an
          array by 2 (mirroring our earlier CPU example). We define the kernel
          with a special <code>@triton.jit</code> decorator and launch it on the
          GPU:
        </p>
        <div class="codehilite">
          <pre><span></span><code><span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mul_kernel</span><span class="p">(</span><span class="n">data_ptr</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">block_start</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK_SIZE</span>                    <span class="c1"># Starting index for data block</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">block_start</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>  <span class="c1"># Indices that block processes</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">offsets</span> <span class="o">&lt;</span> <span class="n">n</span>                                <span class="c1"># Exclude out-of-bounds items</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">data_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>        <span class="c1"># Load items from global memory</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">data_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>        <span class="c1"># Store result to global memory</span>


<span class="c1"># Create array of 160,000 elements and launch kernel</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">160000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">meta</span><span class="p">:</span> <span class="p">(</span><span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">meta</span><span class="p">[</span><span class="s1">&#39;BLOCK_SIZE&#39;</span><span class="p">]),)</span>
<span class="n">mul_kernel</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="n">BLOCK_SIZE</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
</code></pre>
        </div>

        <p>
          Here Triton handles the boilerplate of launching a grid of thread
          blocks. We specify a block size of 1024 threads. The
          <code>pid = tl.program_id(axis=0)</code> gives the block index, and
          then we compute a vector of 1024 offsets for that block. The
          <code>tl.arange(0, BLOCK_SIZE)</code> construct is like getting a SIMD
          lane ID for each thread in the block, and <code>tl.load</code>/<code
            >tl.store</code
          >
          operate on all those lanes in parallel with proper masking for
          out-of-bounds. Essentially, each Triton block processes 1024 elements
          at a time, and Triton figures out how many blocks to launch to cover
          all 160,000 elements.
        </p>
        <p>
          Triton's goal is to be portable across GPU vendors (and potentially
          other accelerators), not just NVIDIA. This is why it leans on MLIR to
          eventually target various backends. The downside is that achieving
          peak performance on every new hardware requires a lot of compiler
          intelligence. It's difficult to encode all the low-level architectural
          quirks in a portable compiler, so hand-tuned vendor-specific libraries
          can still have an edge. In practice, Triton often gets close-to-native
          performance but not absolute maximum. For example, optimized Triton
          kernels have achieved around 75% of the throughput of equivalent cuDNN
          (NVIDIA's highly optimized library) kernels. In one benchmark on
          transformer-layer operations, Triton implementations were typically
          within 1.2–1.5× of CUDA's performance (some nearly equal). This is a
          great trade-off for many cases: you write far less low-level code and
          still get most of the performance.
        </p>
        <h2>First-class Parallelism</h2>
        <p>Almost.</p>
        <p>
          One observation about the state of parallel programming: we've been
          bolting parallelism onto fundamentally single-threaded languages.
          Threads, vector intrinsics, CUDA kernels – these are usually provided
          via libraries, extensions, or compiler tricks layered on top of a base
          language model that is sequential. As a result, developers have to
          constantly switch mental models between "normal" serial code and
          parallel constructs (which often feel foreign to the language).
          Wouldn't it be nice if parallelism were a native feature of the
          language itself?
        </p>
        <p>
          Mojo is a new language (in development) that attempts to do exactly
          that. Created by Chris Lattner (known for LLVM, Clang, Swift, and
          MLIR), Mojo is built with parallel programming as a first-class
          concern. It uses the MLIR infrastructure under the hood to enable
          portable, high-performance compilation to various targets (similar in
          spirit to what Triton does, but in the context of a full language).
          Mojo aims to unify concepts of CPU parallelism (threads, SIMD) and GPU
          programming in a single programming model. Some highlights of Mojo's
          design:
        </p>
        <ol>
          <li>
            It has built-in SIMD vector types and an API to explicitly vectorize
            code (for example, a vectorize function can apply a given operation
            in SIMD across a specified width).
          </li>
          <li>
            The type system differentiates between data in host memory and
            device (accelerator) memory, catching mistakes at compile time and
            easing portability.
          </li>
          <li>
            In general, many parallel and asynchronous constructs are part of
            the core language syntax, not ad-hoc add-ons.
          </li>
        </ol>
        <p>
          To illustrate Mojo's approach, consider again the task of doubling
          each element in an array. In plain Python, you'd write a simple loop:
        </p>
        <div class="codehilite">
          <pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">mul_py</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
        <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">2</span>
</code></pre>
        </div>

        <p>
          If you run this in CPython, it's scalar and single-threaded (the
          interpreter will not use SIMD or multiple cores). Mojo, by contrast,
          can JIT compile a similar looking loop but auto-vectorize it and use
          compiler optimizations transparently. The Mojo version might look
          like:
        </p>
        <div class="codehilite">
          <pre><span></span><code><span class="n">fn</span> <span class="n">mul_mojo</span><span class="p">(</span><span class="n">mut</span> <span class="n">data</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Int32</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">item</span><span class="p">[]</span> <span class="o">*=</span> <span class="mi">2</span>
</code></pre>
        </div>

        <p>
          In Mojo, <code>fn</code> declares a function, and the loop over
          <code>data</code> is auto-vectorized—<code>item[]</code> performs an
          explicit dereference. Unlike Python, this yields a deterministic ~15×
          speedup by guaranteeing SIMD execution without relying on
          pseudo-indeterministic compiler heuristics or manual intrinsics.
        </p>
        <p>
          Mojo is still a work in progress (as of 2025, the GPU programming API
          is highly unstable and not publicly available yet). However, it's a
          promising direction. The plan is that you will be able to write
          portable GPU kernels in Mojo, and the compiler will handle mapping
          them to the hardware efficiently, just like it does for CPU SIMD. Mojo
          is an example of treating parallelism as a foundational feature of the
          programming model rather than an afterthought.
        </p>
        <p>
          Another emerging project in this vein is Bend/HVM, a functional
          heterogeneous programming language.
        </p>
        <h2>Resources</h2>
        <p>
          [0] Introduction to High Performance Scientific Computing – Victor
          Eijkhout
        </p>
        <p>
          [1]
          <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/"
            >https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a
          >
        </p>
        <p>
          [2]
          <a
            href="https://www.kapilsharma.dev/posts/deep-dive-into-triton-internals/"
            >https://www.kapilsharma.dev/posts/deep-dive-into-triton-internals/</a
          >
        </p>
        <p>
          [3]
          <a href="https://docs.modular.com/mojo/manual/"
            >https://docs.modular.com/mojo/manual/</a
          >
        </p>
      </div>
    </div>
  </body>
</html>
